{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f242c785",
   "metadata": {},
   "source": [
    "Héctor Asorey de Pablos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac0ce3",
   "metadata": {},
   "source": [
    "# APRENDIZAJE AUTOMÁTICO II - TRABAJO 2\n",
    "\n",
    "Para esta práctica se ha optado por utilizar aprendizaje por refuerzo para completar el nivel 1-1 del juego Super Mario Bros para la Nintendo Entertainment System, lanzado en 1885.\n",
    "\n",
    "Para ello, se va a utilizar el entorno de Gym \"gym_super_mario_bros\", y se van a entrenar modelos DQNAgent, Double DQNAgent y PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783fc92",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8577e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Crear el entorno del juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8a6d0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Modelos DQNAgent o DDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8ae10",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Definir parámetros del entorno para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79584a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(RIGHT_ONLY)\n",
    "height, width, n_channels = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a636",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Construir modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817187f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(5, height, width, n_channels)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(n_actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1452a13",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Definir la memoria y la política de exploración del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc43753",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=5)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d64a3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Construir el modelo DQNAgent o DDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46988f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=n_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=4000,#50000,\n",
    "               target_model_update=2000,#10000,\n",
    "               policy=policy,\n",
    "               #Quitar la línea de abajo en caso de no querer Double DQN\n",
    "               enable_double_dqn = True)\n",
    "\n",
    "dqn.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429bb11f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=33000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5efae2",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Forma alternativa de entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edbbe0",
   "metadata": {},
   "source": [
    "Se puede entrenar el modelo de forma distinta para saber el reward que obtiene el modelo a cada paso que da gracias a un callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909864d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Definición del callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c96302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintRewardCallback(tf.keras.callbacks.Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        if 'reward' in logs:\n",
    "            print(f\"Step {step}: reward = {logs['reward']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da88e1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=42000, visualize=False, verbose=2, callbacks=[PrintRewardCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ddda0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mario_33_dqn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be19da1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Probar el desempeño del modelo en el juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "modelToTest = load_model('mario_dqn_2000.h5')\n",
    "\n",
    "# Define the memory buffer and the exploration policy\n",
    "memory = SequentialMemory(limit=1000000, window_length=3)\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Create the DQN agent\n",
    "dqn = DQNAgent(model=modelToTest,\n",
    "               nb_actions=n_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=0,\n",
    "               target_model_update=1000,\n",
    "               policy=policy,\n",
    "               enable_double_dqn = True)\n",
    "\n",
    "# Compile the DQN agent\n",
    "dqn.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4))\n",
    "\n",
    "# Evaluate the agent for 10 episodes\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73227dd9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Reentrenar el modelo donde lo dejó en caso de ser necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('mario_33_dqn.h5')\n",
    "\n",
    "# Define the memory buffer and the exploration policy\n",
    "memory = SequentialMemory(limit=1000000, window_length=5)\n",
    "policy = EpsGreedyQPolicy()\n",
    "\n",
    "# Create the DQN agent\n",
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=n_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=0,\n",
    "               target_model_update=1000,\n",
    "               policy=policy)\n",
    "\n",
    "# Compile the DQN agent\n",
    "dqn.compile(optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4))\n",
    "\n",
    "# Evaluate the agent for 10 episodes\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf656a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Modelo PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab7a97",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "### Importar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3633b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = PPO('CnnPolicy', env, verbose=1, tensorboard_log='./logs/', learning_rate=0.000001, \n",
    "            n_steps=512) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0448d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Creación de callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe84afe",
   "metadata": {},
   "source": [
    "Este callback guarda el modelo cada n pasos definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}_v4'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "callback = TrainAndLoggingCallback(check_freq=100000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cefe59",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acec871",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.learn(total_timesteps=200000, callback = callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289d30b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model_20000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c4b40",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Cargar y probar el modelo entrenado en el juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc54f61",
   "metadata": {},
   "outputs": [],
   "source": [
    " model3 = PPO.load('./train/best_model_100000_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f50b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "\n",
    "# Loop through the game\n",
    "while True: \n",
    "    # Make a copy of the state array\n",
    "    state_copy = state.copy()\n",
    "\n",
    "    # Predict the action\n",
    "    action, _ = model3.predict(state_copy)\n",
    "\n",
    "    # Take the action in the environment\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aad8e0",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Reentrenar el modelo PPO donde lo dejó en caso de ser necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "model3.set_env(env)\n",
    "\n",
    "model3.learn(total_timesteps=200000, callback = callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
